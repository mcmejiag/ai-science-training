{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwO5CLF++Y7WaDS7RliNIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcmejiag/ai-science-training/blob/Homeworks/AI_Acelerators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Homework 7 Solutions**\n",
        "1. What are the key architectural features that make these systems suitable for AI workloads?\n",
        "2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.\n",
        "3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?\n",
        "4. Give an example of a project that would benefit from AI accelerators and why?"
      ],
      "metadata": {
        "id": "rq6XMKAy-EX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What are the key architectural features that make these systems suitable for AI workloads?"
      ],
      "metadata": {
        "id": "skulixi_IsXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specialized Hardware Design:** AI accelerators are designed specifically to handle the operations typical in AI workloads, such as matrix multiplications and tensor operations. For instance, Cerebras CS-2 has 850,000 cores optimized for sparse linear algebra, which is crucial for handling compute-intensive models that you can find working in AI workloads. Systems like Cerebras CS-2  are designed specifically to handle the compute-intensive required  for deep learning and other AI applications.\n",
        "\n",
        "**High Memory Bandwidth and Large On-Chip Memory:** This feature is critical for accelerating memory-intensive AI workloads. For instance, Cerebras boasts 40 GB of on-chip memory, and SambaNova features extensive memory configurations, which helps accelerate memory-intensive AI workloads by reducing latency and energy cost associated with data movement.\n",
        "\n",
        "**Scalability and Parallelism:**  Spatial and Reconfigurable Architectures allow for the direct mapping of AI models onto the hardware, enhancing performance by reducing the complexity and cost of traditional computing paradigms. Systems like Graphcore’s IPU use a multitude of processing tiles (each with its own core and local memory), facilitating parallel processing through Bulk Synchronous Parallelism (BSP). This allows efficient handling of computation and communication phases with their unique designs that facilitate parallel processing and deterministic execution. This parallel procesing is use for the AI workloads.\n"
      ],
      "metadata": {
        "id": "2H9lnNP5I8rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models."
      ],
      "metadata": {
        "id": "BOdYeAbRJ2_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SambaNova's Reconfigurable Dataflow Units (RDU):** SambaNova has designed a versatile system that's great for handling big data tasks. It features a layered memory system that can deal with massive amounts of data effortlessly, making it perfect for managing large-scale information. The system’s programming is handled by SambaNova’s own software called SambaFlow, which works smoothly with common AI tools to improve data-focused tasks.\n",
        "\n",
        "**Cerebras Wafer-Scale Engine (WSE):** Cerebras stands out with its Wafer-Scale Engine, which is essentially a large collection of processing units, each with its own memory and the ability to function independently. This design improves the engine's ability to manage multiple tasks at once, making it both fast and scalable. It’s particularly good with popular tools like TensorFlow and PyTorch, thanks to specialized software that makes the most of the engine’s unique setup.\n",
        "\n",
        "**Graphcore IPU:** Graphcore’s IPU is made up of numerous connected units, each equipped with its own processing power and memory. This setup is crafted to handle two major tasks—processing data and then sharing it—using a method called Bulk Synchronous Parallelism. Graphcore uses a special toolkit called Poplar SDK to ensure everything runs smoothly and efficiently, balancing the workload between the units.\n",
        "\n",
        "**Groq’s Tensor Streaming Processor (TSP):** Groq's architecture is built around its Tensor Streaming Processor, focusing on delivering consistent and predictable results. This is particularly important for AI tasks where timing is crucial. By avoiding the usual complications of dynamic memory, Groq’s design ensures steady performance. Its software supports this reliable execution style, making it easier to run AI models that need fast and steady processing speeds.\n",
        "\n"
      ],
      "metadata": {
        "id": "id7CNFtAJ6lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?"
      ],
      "metadata": {
        "id": "ahkKUnm3ZzqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When setting up an AI model, you'd typically start with a framework like PyTorch, adjusting it to get the best out of the specific hardware you're using. Once you've fine-tuned the model, you'll use specialized software, like Cerebras SDK, to make sure the model can run smoothly on your chosen hardware; this involves transforming the model into the right format and optimizing it further. For example, moving from PyTorch to PopTorch or using SambaFlow for model conversion are common steps in this process. After everything is set up, you’ll compile the model using something like the Cerebras Graph Compiler and then load it directly onto the hardware for execution."
      ],
      "metadata": {
        "id": "nmUfLFVvZ5iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Give an example of a project that would benefit from AI accelerators and why?"
      ],
      "metadata": {
        "id": "jQBoNUBub9f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Training large-scale neural networks for image recognition or natural language processing. These tasks require handling vast amounts of data and compute operations which AI accelerators can speed up significantly. The specialized hardware like Cerebras or SambaNova can perform these tasks much faster than traditional CPUs or even GPUs, reducing training time from weeks to days or hours, accelerating the cycle of model development and deployment."
      ],
      "metadata": {
        "id": "fyT6bq1z-Jj8"
      }
    }
  ]
}